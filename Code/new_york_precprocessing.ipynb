{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87643d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /N/u/pamal/Carbonate/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /N/u/pamal/Carbonate/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /N/u/pamal/Carbonate/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6784a0f6b6ea47a6af5dfd913bacd007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4986461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "import swifter\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the data\n",
    "comments_df = pd.read_csv('nyt-comments-2020.csv', low_memory=False)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove non-alphabetic characters and punctuations\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.translate(str.maketrans('', '', string.punctuation)))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the comment body column using Swifter for faster processing\n",
    "comments_df['cleaned_comment'] = comments_df['commentBody'].swifter.apply(lambda x:preprocess(x))\n",
    "# Save the cleaned data and bag of words matrix to CSV files\n",
    "comments_df.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b8d85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4985131, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "comments_df = pd.read_csv('cleaned_data.csv', low_memory=False)\n",
    "# Remove rows with missing values\n",
    "\n",
    "comments_df=comments_df[['cleaned_comment','editorsSelection']]\n",
    "comments_df['cleaned_comment'].replace({'':np.nan},inplace=True)\n",
    "comments_df.dropna(inplace=True,axis=0)\n",
    "comments_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc8d22b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4985131/4985131 [02:19<00:00, 35615.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4985131, 208)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create a bag of words matrix using CountVectorizer with a minimum document frequency of 2.5%\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=int(0.025 * len(comments_df)))\n",
    "bag_of_words_matrix = vectorizer.fit_transform(tqdm(comments_df['cleaned_comment']))\n",
    "bag_of_words_df = pd.DataFrame.sparse.from_spmatrix(bag_of_words_matrix, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Save bag of words matrix to CSV file\n",
    "bag_of_words_df.to_csv('bag_of_words.csv', index=False)\n",
    "bag_of_words_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dcc0dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2983, 24)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8bdb05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96177da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
